{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 图像分类是根据图像的语义信息将不同类别图像区分开来，是计算机视觉中重要的基本问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1、准备数据**\n",
    "\n",
    "**2、配置网络**\n",
    "\n",
    "\t （1）定义网络\n",
    "     \n",
    "     （2）定义损失函数\n",
    "     \n",
    "     （3）定义优化算法\n",
    "     \n",
    "**3、训练网络**\n",
    "\n",
    "**4、模型评估**\n",
    "\n",
    "**5、模型预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n",
      "2021-03-29 11:10:32,094 - INFO - font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2021-03-29 11:10:32,433 - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "#导入必要的包\n",
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "import paddle\n",
    "import matplotlib.pyplot as plt\n",
    "from paddle.nn import MaxPool2D,Conv2D,BatchNorm\n",
    "from paddle.nn import Linear\n",
    "import sys\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import paddle\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据集介绍**\n",
    "\n",
    "台湾电力公司、台湾海洋研究所和垦丁国家公园在2010年10月1日至2013年9月30日期间，在台湾南湾海峡、兰屿岛和胡比湖的水下观景台收集的鱼类图像数据集。\n",
    "\n",
    "该数据集包括23类鱼种，共27370张鱼的图像。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b7213f7d65c54b6fae98d57de58e3d2ad29b68abc68140b4b283c615144cd40b)\n",
    "\n",
    "本实践选取5种鱼类数据作为数据集进行训练，被划分为两个子集，训练集和测试集比例为9:1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# **step1、准备数据：**\n",
    "\n",
    "**1、生成数据列表**\n",
    "\n",
    "**2、定义数据提供器 train_r、test_r**\n",
    "\n",
    "**3、定义train_reader、test_reader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#解压原始数据集，将fish_image.zip解压至data目录下\n",
    "src_path=\"/home/aistudio/data/data14492/fish_image23.zip\"\n",
    "target_path=\"/home/aistudio/data/fish_image\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#存放所有类别的信息\n",
    "class_detail = []\n",
    "#获取所有类别保存的文件夹名称\n",
    "class_dirs = os.listdir(target_path+\"/fish_image\")\n",
    "\n",
    "data_list_path=\"/home/aistudio/data/\"\n",
    "\n",
    "TRAIN_LIST_PATH=data_list_path + \"train.txt\"\n",
    "EVAL_LIST_PATH=data_list_path + \"eval.txt\"\n",
    "\n",
    "#每次执行代码，首先清空train.txt和eval.txt\n",
    "with open(TRAIN_LIST_PATH, 'w') as f: \n",
    "    pass\n",
    "with open(EVAL_LIST_PATH, 'w') as f: \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成数据列表完成！\n"
     ]
    }
   ],
   "source": [
    "#总的图像数量\n",
    "all_class_images = 0\n",
    "#存放类别标签\n",
    "class_label=0\n",
    "# 设置要生成文件的路径\n",
    "data_root_path=\"/home/aistudio/data/fish_image/fish_image\"\n",
    "#存储要写进test.txt和train.txt中的内容\n",
    "trainer_list=[]\n",
    "eval_list=[]\n",
    "#读取每个类别，['fish_01', 'fish_02', 'fish_03']\n",
    "for class_dir in class_dirs:   \n",
    "    #每个类别的信息\n",
    "    class_detail_list = {}\n",
    "    eval_sum = 0\n",
    "    trainer_sum = 0\n",
    "    #统计每个类别有多少张图片\n",
    "    class_sum = 0\n",
    "    #获取类别路径 \n",
    "    path = data_root_path + \"/\" + class_dir\n",
    "    # 获取所有图片\n",
    "    img_paths = os.listdir(path)\n",
    "    for img_path in img_paths:                                  # 遍历文件夹下的每个图片\n",
    "        name_path = path + '/' + img_path                       # 每张图片的路径\n",
    "        if class_sum % 10 == 0:                                 # 每10张图片取一个做验证数据\n",
    "            eval_sum += 1                                       # test_sum为测试数据的数目\n",
    "            eval_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")\n",
    "        else:\n",
    "            trainer_sum += 1 \n",
    "            trainer_list.append(name_path + \"\\t%d\" % class_label + \"\\n\")#trainer_sum测试数据的数目\n",
    "        class_sum += 1                                          #每类图片的数目\n",
    "        all_class_images += 1                                   #所有类图片的数目\n",
    "    class_label += 1  \n",
    "    # 说明的json文件的class_detail数据\n",
    "    class_detail_list['class_name'] = class_dir             #类别名称，如jiangwen\n",
    "    class_detail_list['class_label'] = class_label          #类别标签\n",
    "    class_detail_list['class_eval_images'] = eval_sum       #该类数据的测试集数目\n",
    "    class_detail_list['class_trainer_images'] = trainer_sum #该类数据的训练集数目\n",
    "    class_detail.append(class_detail_list)         \n",
    "    \n",
    "random.shuffle(eval_list)\n",
    "with open(data_list_path + \"eval.txt\", 'a') as f:\n",
    "    for eval_image in eval_list:\n",
    "        f.write(eval_image) \n",
    "        \n",
    "random.shuffle(trainer_list)\n",
    "with open(data_list_path + \"train.txt\", 'a') as f2:\n",
    "    for train_image in trainer_list:\n",
    "        f2.write(train_image) \n",
    "\n",
    "# 说明的json文件信息\n",
    "readjson = {}\n",
    "readjson['all_class_name'] = data_root_path                  #文件父目录\n",
    "readjson['all_class_sum'] = class_sum \n",
    "readjson['all_class_images'] = all_class_images\n",
    "readjson['class_detail'] = class_detail\n",
    "jsons = json.dumps(readjson, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "with open(data_list_path + \"readme.json\",'w') as f:\n",
    "    f.write(jsons)\n",
    "print ('生成数据列表完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/9d3504947dca47b7bb0bfd50700d4ff4c804e7c344be496bbdfc0d911e66b58f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "import paddle.vision.transforms as T\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "\r\n",
    "class FishDataset(paddle.io.Dataset):\r\n",
    "    \"\"\"\r\n",
    "    23种鱼数据集类的定义\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        初始化函数\r\n",
    "        \"\"\"\r\n",
    "        assert mode in ['train', 'eval'], 'mode is one of train, eval.'\r\n",
    "\r\n",
    "        self.data = []\r\n",
    "\r\n",
    "        with open('data/{}.txt'.format(mode)) as f:\r\n",
    "            for line in f.readlines():\r\n",
    "                info = line.strip().split('\\t')\r\n",
    "\r\n",
    "                if len(info) > 0:\r\n",
    "                    self.data.append([info[0].strip(), info[1].strip()])\r\n",
    "\r\n",
    "        if mode == 'train':\r\n",
    "            self.transforms = T.Compose([\r\n",
    "                T.Resize((47,47)),    # 随机裁剪大小\r\n",
    "                T.RandomHorizontalFlip(0.5),        # 随机水平翻转\r\n",
    "                T.ToTensor(),                       # 数据的格式转换和标准化、 HWC => CHW \r\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 图像归一化\r\n",
    "            ])\r\n",
    "        else:\r\n",
    "            self.transforms = T.Compose([\r\n",
    "                T.Resize((47,47)),                 # 图像大小修改\r\n",
    "                T.ToTensor(),                  # 数据的格式转换和标准化 HWC => CHW\r\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])   # 图像归一化\r\n",
    "            ])\r\n",
    "        \r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        根据索引获取单个样本\r\n",
    "        \"\"\"\r\n",
    "        image_file, label = self.data[index]\r\n",
    "        image = Image.open(image_file)\r\n",
    "\r\n",
    "        if image.mode != 'RGB':\r\n",
    "            image = image.convert('RGB')\r\n",
    "\r\n",
    "        image = self.transforms(image)\r\n",
    "\r\n",
    "        return image, np.array(label, dtype='int64')\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \"\"\"\r\n",
    "        获取样本总数\r\n",
    "        \"\"\"\r\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset=FishDataset(mode='train')\r\n",
    "eval_dataset=FishDataset(mode='eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# step2、配置网络\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1be52cfb6fc24d61995f9313990827a61bd9eb0d92d04eb39edb990770411875)\n",
    "\n",
    "\n",
    "**卷积层：执行卷积操作提取底层到高层的特征，发掘出图片“局部特性”；**\n",
    "\n",
    "**池化层：通过降采样的方式，在不影响图像质量的情况下，压缩图片，减少参数；**\n",
    "\n",
    "**全连接层：池化完成后，将数据“拍平”，丢到Flatten层，然后把Flatten层的输出放到全连接层里，可采用softmax对其进行分类。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#定义CNN网络\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class MyCNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyCNN,self).__init__()\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=20, kernel_size=5,stride=1)        \n",
    "        self.pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=50, kernel_size=5,stride=1)\n",
    "        self.pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.conv3 = Conv2D(in_channels=50, out_channels=50, kernel_size=5,stride=1)\n",
    "        self.pool3 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        self.linear1 = Linear(in_features=200, out_features=23)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = paddle.flatten(x, start_axis=1,stop_axis=-1)  \n",
    "        x = self.linear1(x)\n",
    "        x=F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# step3、训练网络&step4、评估网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import Model\n",
    "\n",
    "model= Model(MyCNN())\n",
    "\n",
    "# 配置模型\n",
    "model.prepare(paddle.optimizer.Adam(learning_rate=0.0005, parameters=model.parameters()), \n",
    "              paddle.nn.CrossEntropyLoss(), \n",
    "              paddle.metric.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10/97 [==>...........................] - loss: 2.9924 - acc: 0.1566 - ETA: 57s - 660ms/step"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=5, batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(eval_dataset, batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# step5模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 进行预测操作\r\n",
    "predict_result = model.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "data=pd.read_csv('./data/eval.txt',header=None,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义画图方法\n",
    "from PIL import Image\n",
    "\n",
    "def show_img(img, predict):\n",
    "    plt.figure()\n",
    "    plt.title(predict)\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "# 抽样展示\n",
    "for i in range(10):\n",
    "    img_path=data[0][i]\n",
    "    real_class=data[1][i]\n",
    "    predict_class= np.argmax(predict_result[0][i])\n",
    "    img=Image.open(img_path)\n",
    "    title=str(i) +'   '+ 'real_class: ' +str(real_class)+'  '+ 'predict_class: ' + str(predict_class)\n",
    "    show_img(img, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('inference_model', training=False)  # save for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
